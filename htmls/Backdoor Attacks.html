<!DOCTYPE html>
<html>

<head>
  <!-- Basic -->
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <!-- Mobile Metas -->
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
  <!-- Site Metas -->
  <meta name="keywords" content="" />
  <meta name="description" content="" />
  <meta name="author" content="" />

  <title>Backdoor Attacks</title>

  <!-- slider stylesheet -->
  <!-- slider stylesheet -->
  <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/OwlCarousel2/2.3.4/assets/owl.carousel.min.css" />

  <!-- bootstrap core css -->
  <link rel="stylesheet" type="text/css" href="css/bootstrap.css" />

  <!-- fonts style -->
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:400,700|Poppins:400,700&display=swap" rel="stylesheet">
  <!-- Custom styles for this template -->
  <link href="css/style.css" rel="stylesheet" />
  <!-- responsive style -->
  <link href="css/responsive.css" rel="stylesheet" />

  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">
</head>

<body>

  <div class="hero_area">
    <!-- header section strats -->
    <header class="header_section">
      <div class="container-fluid">
        <nav class="navbar navbar-expand-lg custom_nav-container pt-3">
          <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
          </button>

          <div class="collapse navbar-collapse" id="navbarSupportedContent">
            <div class="d-flex ml-auto flex-column flex-lg-row align-items-center">
              <ul class="navbar-nav  ">
                <li class="nav-item active">
                  <a class="nav-link" href="../index.html">Home <span class="sr-only">(current)</span></a>
                </li>
                <li class="nav-item">
                  <a class="nav-link" href="Adversarial Attacks.html"> Adversarial Attack </a>
                </li>
                <!-- <li class="nav-item">
                  <a class="nav-link" href="do.html"> What we do </a>
                </li>
                <li class="nav-item">
                  <a class="nav-link" href="portfolio.html"> Portfolio </a>
                </li> -->
                <!-- <li class="nav-item">
                  <a class="nav-link" href="contact.html">Contact us</a>
                </li> -->
              </ul>
              <div class="user_option">
                <a href="member.html" style="color: #fff;">
                  <img src="images/user.png" alt="" style="height: 25px;">Group Members
                </a>
                <!-- <form class="form-inline my-2 my-lg-0 ml-0 ml-lg-4 mb-3 mb-lg-0">
                  <button class="btn  my-2 my-sm-0 nav_search-btn" type="submit"></button>
                </form> -->
              </div>
            </div>
          </div>
        </nav>
      </div>
    </header>
    <!-- end header section -->
    <!-- slider section -->
    <section class=" slider_section position-relative">
      <div class="container">
        <div id="carouselExampleIndicators" class="carousel slide" data-ride="carousel">
          <ol class="carousel-indicators"> 
            <li data-target="#carouselExampleIndicators" data-slide-to="0" class="active"></li>
            <li data-target="#carouselExampleIndicators" data-slide-to="1"></li>
            <li data-target="#carouselExampleIndicators" data-slide-to="2"></li>
          </ol> 

          <div class="carousel-inner">
            <div class="carousel-item active">
              <div class="row">
                <div class="col">
                  <div class="detail-box">
                    <div>
                      <h1>
                        Backdoor Attacks
                      </h1>
                      <p>
                        &nbsp;&nbsp;&nbsp;&nbsp;Backdoor attack is a type of highly detrimental poisoning attack that allows
                        the attacker to implant a "backdoor" into a model, and during the model application phase, malicious
                        actions are completed through simple backdoor triggers. The neural model implanted with
                        the backdoor behave normally on benign samples but make specific erroneous predictions on
                        input samples with the particular backdoor trigger. The backdoor can persist indefinitely within
                        neural network and remain concealed until activated by samples carrying the specific backdoor
                        trigger. Thus, backdoor attacks possess a high level of stealthiness, posing serious security
                        risks to many se curity-related applications.
                      </p>
                      <!-- <div class="img-box">
                        <img src="images/backdoor-workflow.png">
                      </div> -->
                      <!-- <div class="">
                        <a href="">
                          Contact us
                        </a>
                      </div> -->

                    </div>
                  </div>
                </div>
              </div>
            </div>

            <div class="carousel-item">
              <div class="row">
                <div class="col">
                  <div class="detail-box">
                    <div>                  
                      <h2>
                        Data Poisoning
                      </h2>
                      <div class="img-box">
                        <img src="images/backdoor-data-poisoning.png">
                      </div>  
                      <p>
                        Data poisoning primarily occurs during the data collection phase before model training. 
                        The premise of data poisoning is that the attacker can obtain a portion of training data. 
                        Attacker first add the trigger to these data as poisoned data, 
                        and then assign specified target output to the poisoned data. 
                        Finally, these poisoned data are published on open platforms, 
                        such as GitHub. Users or developers collect those poisoned data and train a model with it alongside clean data. 
                        For samples with the trigger, the model learns the features of the backdoor pattern. 
                        For any input containing the backdoor trigger, the model predicts the attacker's specified target output. 
                        For clean data, the model still learns the features of the data itself and can output correct predictions.
                      </p>
                    </div>  
                  </div>
                </div>
              </div>
            </div>

            <div class="carousel-item">
              <div class="row">
                <div class="col">
                  <div class="detail-box">
                    <h2>
                      Model Poisoning
                    </h2>
                    <div class="img-box">
                      <img src="images/backdoor-model-poisoning.png">
                    </div>
                    <p>
                      Model poisoning primarily occurs during the model training phase. The main distinction between model poisoning and data poisoning lies in the attacker's ability to control the model's training process. Similar to data poisoning, attacker first poisons the training data. Subsequently, the poisoned data are used to train a poisoned pre-trained model, which is then published on a open platform such as Hugging Face. Users or developers download and use these poisoned the pre-trained model for training or fine-tuning a downstream task model as needed. Research indicates that even after fine-tuning with clean data, the model still contain backdoors implanted by the attacker. Consequently, the attacker can launch attacks on the downstream task model using inputs containing the backdoor trigger, resulting in the output of the attacker's desired results. 
                    </p>
                  </div>
                </div>
              </div>
            </div>

          </div>
        </div>

      </div>
    </section>
    <!-- end slider section -->
  </div>

  <div style="background-color: #f0f0f0; background-image: url(images/grey-background.png);">
        <!-- end work section -->
    <div class="table-container">
      <section class="who_section">
        <div class="container">
          <div class="row">
            <div class="col-md-12">
                <!-- 内容展示区域 -->
                <div id="contentArea" class="content-item">
                    <!-- 动态加载内容 -->
                </div>
            </div>
          </div>

          <div class="row">
              <div class="col-md-12">
                  <!-- 分页组件 -->
                  <nav aria-label="Pagination example">
                      <ul class="pagination" id="pagination">
                        <!-- 分页链接将在这里动态生成 -->
                      </ul>
                  </nav>
              </div>
          </div>

        </div>
      </section>
    </div>

    <div class="table-container">
      <section class="table">
        <div class="container mt-5">
          <h2>Datasets used in backdoor research on CodeLMs</h2>
          <table class="table table-bordered table-striped table-hover">
            <thead class="thead-dark">
              <tr>
                <th scope="col">Dataset</th>
                <th scope="col">Year</th>
                <th scope="col">Programming Language</th>
                <th scope="col">Data Source</th>
                <th scope="col">Download Link</th>
                <th scope="col">Study</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>BigCloneBench</td>
                <td>2014</td>
                <td>Java</td>
                <td>GitHub</td>
                <td><a href="https://github.com/clonebench/BigCloneBench">Download</a></td>
                <td>XXX</td>
              </tr>
              <!-- 其他行数据 -->
              <!-- ... -->
              <tr>
                <td>OJ dataset</td>
                <td>2016</td>
                <td>C++</td>
                <td> OJ Platform</td>
                <td> <a href="http://programming.grids.cn">Download</a></td>
                <td>XXX</td>
              </tr>

              <tr>
                <td>CodeSearchNet</td>
                <td>2019</td>
                <td>Go <br>
                    Java <br>
                    JavaScript <br>
                    PHP <br>
                    Python <br>
                    Ruby <br>
                </td>
                <td> GitHub </td>
                <td> <a href=" https://github.com/github/CodeSearchNet">Download</a></td>
                <td>XXX</td>
              </tr>
              <tr>
                <td>Code2Seq</td>
                <td>2019</td>
                <td>Java
                </td>
                <td> GitHub </td>
                <td> <a href=" https://github.com/tech-srl/code2seq#datasets">Download</a></td>
                <td>XXX</td>
              </tr>          
              <tr>
                <td>Devign</td>
                <td>2019</td>
                <td>Java
                </td>
                <td> GitHub </td>
                <td> <a href=" https://github.com/rjust/defects4j">Download</a></td>
                <td>XXX</td>
              </tr>       
              <tr>
                <td>Google Code Jam (GCJ)</td>
                <td>2020</td>
                <td>C++ <br>
                    Java
                </td>
                <td> OJ Platform </td>
                <td> <a href=" https://codingcompetitions.withgoogle.com/codejam">Download</a></td>
                <td>XXX</td>
              </tr>       
              <tr>
                <td>CodeXGLUE</td>
                <td>2021</td>
                <td>Go <br>
                    Java <br>
                    JavaScript <br>
                    PHP <br>
                    Python <br>
                    Ruby <br>
                </td>
                <td> GitHub </td>
                <td> <a href=" https://github.com/microsoft/CodeXGLUE">Download</a></td>
                <td>XXX</td>
              </tr>       
              <tr>
                <td>CodeQA</td>
                <td>2021</td>
                <td>Java <br>
                    Python
                </td>
                <td> GitHub </td>
                <td> <a href=" https://github.com/jadecxliu/codeqa">Download</a></td>
                <td>XXX</td>
              </tr>       
              <tr>
                <td>APPS</td>
                <td>2021</td>
                <td>Python
                </td>
                <td> OJ Platform </td>
                <td> <a href=" https://people.eecs.berkeley.edu/hendrycks/APPS.tar.gz">Download</a></td>
                <td>XXX</td>
              </tr>      
              <tr>
                <td>Shellcode_IA32</td>
                <td>2021</td>
                <td>assembly language instruction
                </td>
                <td> OJ Platform </td>
                <td> <a href=" https://github.com/dessertlab/Shellcode_IA32">Download</a></td>
                <td>XXX</td>
              </tr>      
              <tr>
                <td>SecurityEval</td>
                <td>2022</td>
                <td>Python
                </td>
                <td> GitHub </td>
                <td> <a href=" https://github.com/s2e-lab/SecurityEval">Download</a></td>
                <td>XXX</td>
              </tr>      
              <tr>
                <td>LLMSecEval</td>
                <td>2023</td>
                <td>Python<br>
                  C
                </td>
                <td> GitHub </td>
                <td> <a href=" https://github.com/tuhh-softsec/LLMSecEval">Download</a></td>
                <td>XXX</td>
              </tr>      
              <tr>
                <td>PoisonPy</td>
                <td>2023</td>
                <td>Python
                </td>
                <td> GitHub </td>
                <td> 
                  not yet published
                </td>
                <td>XXX</td>
              </tr>

            </tbody>
          </table>
        </div>
      </section>
    </div>
    <!-- target model section -->
    <div class="table-container">
      <section class="table">
        <div class="container mt-5">
          <h2> A summary of target models of backdoor attacks in CodeLMs</h2>
          <table class="table table-bordered table-striped table-hover">
            <thead class="thead-dark">
              <tr>
                <th scope="col">Attack Technique</th>
                <th scope="col">Year</th>
                <th scope="col">Venue</th>
                <th scope="col">Attack Type</th>
                <th scope="col">Target Models</th>
                <th scope="col">Target Tasks</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>Remakrishnan et al.</td>
                <td>2020</td>
                <td>arXiv</td>
                <td> Data poisoning</td>
                <td>Code2Seq <br>
                  Seq2Seq</td>
                <td>Code summarization<br>
                  Method name prediction</td>
              </tr>
              <!-- 其他行数据 -->
              <!-- ... -->
              <tr>
                <td>Schuster et al.</td>
                <td>2021</td>
                <td>USENIX Security</td>
                <td>Data poisoning<br>Model poisoning</td>
                <td>Pythia <br>
                  GPT-2</td>
                <td>Code completion</td>
              </tr>

              <tr>
                <td>Severi et al.</td>
                <td>2021</td>
                <td>USENIX Security</td>
                <td>Data poisoning</td>
                <td>LightGBM <br>
                  EmberNN <br>
                  Random Forest <br>
                  Linear SVM</td>
                <td>Malware classification</td>
              </tr>

              <tr>
                <td>CodePoisoner</td>
                <td>2022</td>
                <td>arXiv</td>
                <td>Data poisoning</td>
                <td>LSTM <br>
                  TextCNN <br>
                  Transformer <br>
                  CodeBERT
                </td>
                <td>Code defect detection<br>
                  Code clone detection<br>
                  Code repair</td>
              </tr>
              
              <tr>
                <td>Wan et al.</td>
                <td>2022</td>
                <td>ESEC/FSE</td>
                <td>Data poisoning</td>
                <td>BiRNN<br>
                  Transformer<br>
                  CodeBERT</td>
                <td>Code search</td>
              </tr>

              <tr>
                <td>BADCODE</td>
                <td>2023</td>
                <td>ACL</td>
                <td>Data poisoning</td>
                <td>CodeBERT<br>
                  CodeT5</td>
                <td>Code search</td>
              </tr>          
              
              <tr>
                <td>Cotroneo et al.</td>
                <td>2023</td>
                <td>arXiv</td>
                <td> Data poisoning</td>
                <td>Seq2Seq<br>
                  CodeBERT<br>
                  CodeT5+</td>
                <td>Code generation</td>
              </tr>          
              
              <tr>
                <td>AFRAIDOOR</td>
                <td>2023</td>
                <td>arXiv</td>
                <td> Data poisoning</td>
                <td>CodeBERT<br>
                  CodeT5<br>
                  PLBART</td>
                <td>Code summarization</td>
              </tr>          
              
              <tr>
                <td>PELICAN</td>
                <td>2023</td>
                <td>USENIX Security</td>
                <td> Data poisoning</td>
                <td>BiRNN-func<br>
                  XDA-func<br>
                  XDA-cell<br>
                  StateFormer<br>
                  EKLAVYA<br>
                  EKLAVYA++<br>
                  in-nomine<br>
                  in-nomine++<br>
                  S2V, S2V++<br>
                  Trex<br>
                  SAFE, SAFE++<br>
                  S2V-B, S2V-B++</td>
                <td>Binary code analysis</td>
              </tr>          
              
              
              <tr>
                <td>Li et al.</td>
                <td>2023</td>
                <td>ACL</td>
                <td>Model poisoning</td>
                <td>PLBART<br>
                  CodeT5</td>
                <td>Code defect detection<br>
                  Code clone prediction<br>
                  Code2Code translation<br>
                  Text2Code translation<br>
                  Code refine</td>
              </tr>          
              
              <tr>
                <td>BadCS</td>
                <td>2023</td>
                <td>arXiv</td>
                <td>Model Poisoning</td>
                <td>BiRNN<br> Transformer<br> CodeBERT<br> GraphCodeBERT</td>
                <td>Code Search</td>
              </tr>
            </tbody>
          </table>
        </div>
      </section>
    </div>
    <!-- end target model section -->
  </div>

  <!-- footer section -->
  <section class="container-fluid footer_section">
    <p>
      &copy; 2024 All Rights Reserved By
      <a href="https://html.design/">The Team</a>
    </p>
  </section>
  <!-- footer section -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>
  <script type="text/javascript" src="js/jquery-3.4.1.min.js"></script>
  <script type="text/javascript" src="js/bootstrap.js"></script>
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/OwlCarousel2/2.3.4/owl.carousel.min.js">
  </script>
  <!-- owl carousel script 
    -->
  <script type="text/javascript">
    $(".owl-carousel").owlCarousel({
      loop: true,
      margin: 0,
      navText: [],
      center: true,
      autoplay: true,
      autoplayHoverPause: true,
      responsive: {
        0: {
          items: 1
        },
        1000: {
          items: 3
        }
      }
    });

  </script>



  <script>
    const jsonData = {
      "papers": [
                {
                  "id": 1,
                  "title": "You Autocomplete Me: Poisoning Vulnerabilities in Neural Code Completion",
                  "authors": "Authors: Mohammed Abuhamad, David Mohaisen,  Changhun Jung, DaeHun Nyang",
                  "imgSrc": "images/class2_papers/codecompletion.png",
                  "description": "Code autocompletion is an integral feature of modern code editors and IDEs. The latest generation of autocompleters uses neural language models, trained on public open-source code repositories, to suggest likely (not just statically feasible) completions given the current context. \
                                  We demonstrate that neural code autocompleters are vulnerable to poisoning attacks. \
                                  By adding a few specially-crafted files to the autocompleter's training corpus (data poisoning), \
                                  or else by directly fine-tuning the autocompleter on these files (model poisoning), \
                                  the attacker can influence its suggestions for attacker-chosen contexts. \
                                  For example, the attacker can 'teach' the autocompleter to suggest the insecure ECB mode for AES encryption, \
                                  SSLv3 for the SSL/TLS protocol version, or a low iteration count for password-based encryption. \
                                  Moreover, we show that these attacks can be targeted: \
                                  an autocompleter poisoned by a targeted attack is much more likely to suggest the insecure completion for files from a specific repo or specific developer. \
                                  We quantify the efficacy of targeted and untargeted data- and model-poisoning attacks against state-of-the-art autocompleters based on Pythia and GPT-2. \
                                  We then evaluate existing defenses against poisoning attacks, and show that they are largely ineffective.",
                  "link": "https://www.usenix.org/system/files/sec21-schuster.pdf"
                },
                {
                  "id": 2,
                  "title": "Explanation-Guided Backdoor Poisoning Attacks Against Malware Classifiers",
                  "authors": "Authors:  Giorgio Severi, Jim Meyer, Scott E. Coull, Alina Oprea",
                  "imgSrc": "images/class2_papers/malware.png",
                  "description": "Training pipelines for machine learning (ML) based malware classification often rely on crowdsourced threat feeds, exposing a natural attack injection point. In this paper, we study the susceptibility of feature-based ML malware classifiers to backdoor poisoning attacks, specifically focusing on challenging 'clean label' attacks where attackers do not control the sample labeling process. We propose the use of techniques from explainable machine learning to guide the selection of relevant features and values to create effective backdoor triggers in a model-agnostic fashion. Using multiple reference datasets for malware classification, including Windows PE files, PDFs, and Android applications, we demonstrate effective attacks against a diverse set of machine learning models and evaluate the effect of various constraints imposed on the attacker. To demonstrate the feasibility of our backdoor attacks in practice, we create a watermarking utility for Windows PE files that preserves the binary's functionality, and we leverage similar behavior-preserving alteration methodologies for Android and PDF files. Finally, we experiment with potential defensive strategies and show the difficulties of completely defending against these attacks, especially when the attacks blend in with the legitimate sample distribution.",
                  "link": "https://www.usenix.org/system/files/sec21-severi.pdf"
                },
                {
                  "id": 3,
                  "title": "Backdooring Neural Code Search",
                  "authors": "Authors: Weisong Sun, Yuchen Chen, Guanhong Tao, Chunrong Fang, Xiangyu Zhang, Quanjun Zhang, Bin Luo",
                  "imgSrc": "images/class2_papers/badcode.png",
                  "description": "Reusing off-the-shelf code snippets from online repositories is a common practice, which significantly enhances the productivity of software developers. To find desired code snippets, developers resort to code search engines through natural language queries. Neural code search models are hence behind many such engines. These models are based on deep learning and gain substantial attention due to their impressive performance. However, the security aspect of these models is rarely studied. Particularly, an adversary can inject a backdoor in neural code search models, which return buggy or even vulnerable code with security/privacy issues. This may impact the downstream software (e.g., stock trading systems and autonomous driving) and cause financial loss and/or life-threatening incidents. In this paper, we demonstrate such attacks are feasible and can be quite stealthy. By simply modifying one variable/function name, the attacker can make buggy/vulnerable code rank in the top 11%. Our attack BADCODE features a special trigger generation and injection procedure, making the attack more effective and stealthy. The evaluation is conducted on two neural code search models and the results show our attack outperforms baselines by 60%. Our user study demonstrates that our attack is more stealthy than the baseline by two times based on the F1 score.",
                  "link": "https://arxiv.org/pdf/2305.17506"
                },
                {
                  "id": 4,
                  "title": "BadCS: A Backdoor Attack Framework for Code search",
                  "authors": "Authors: Shiyi Qi, Yuanhang Yang, Shuzhzeng Gao, Cuiyun Gao, Zenglin Xu",
                  "imgSrc": "images/class2_papers/badcs.png",
                  "description": "With the development of deep learning (DL), DL-based code search models have achieved state-of-the-art performance and have been widely used by developers during software development. However, the security issue, e.g., recommending vulnerable code, has not received sufficient attention, which will bring potential harm to software development. Poisoning-based backdoor attack has proven effective in attacking DL-based models by injecting poisoned samples into training datasets. However, previous work shows that the attack technique does not perform successfully on all DL-based code search models and tends to fail for Transformer-based models, especially pretrained models. Besides, the infected models generally perform worse than benign models, which makes the attack not stealthy enough and thereby hinders the adoption by developers. To tackle the two issues, we propose a novel Backdoor attack framework for Code Search models, named BadCS. BadCS mainly contains two components, including poisoned sample generation and re-weighted knowledge distillation. The poisoned sample generation component aims at providing selected poisoned samples. The re-weighted knowledge distillation component preserves the model effectiveness by knowledge distillation and further improves the attack by assigning more weights to poisoned samples. Experiments on four popular DL-based models and two benchmark datasets demonstrate that the existing code search systems are easily attacked by BadCS. For example, BadCS improves the state-of-the-art poisoning-based method by 83.03%-99.98% and 75.98%-99.90% on Python and Java datasets, respectively. Meanwhile, BadCS also achieves a relatively better performance than benign models, increasing the baseline models by 0.49% and 0.46% on average, respectively.",
                  "link": "https://arxiv.org/pdf/2305.05503"
                },
                {
                  "id": 5,
                  "title": "Stealthy Backdoor Attack for Code Models",
                  "authors": "Authors: Zhou Yang, Bowen Xu, Jie M. Zhang, Hong Jin Kang, Jieke Shi, Junda He, David Lo",
                  "imgSrc": "images/class2_papers/yangzhou.png",
                  "description": "Code models, such as CodeBERT and CodeT5, offer general-purpose representations of code and play a vital role in supporting downstream automated software engineering tasks. Most recently, code models were revealed to be vulnerable to backdoor attacks. A code model that is backdoor-attacked can behave normally on clean examples but will produce pre-defined malicious outputs on examples injected with triggers that activate the backdoors. Existing backdoor attacks on code models use unstealthy and easy-to-detect triggers. This paper aims to investigate the vulnerability of code models with stealthy backdoor attacks. To this end, we propose AFRAIDOOR (Adversarial Feature as Adaptive Backdoor). AFRAIDOOR achieves stealthiness by leveraging adversarial perturbations to inject adaptive triggers into different inputs. We evaluate AFRAIDOOR on three widely adopted code models (CodeBERT, PLBART and CodeT5) and two downstream tasks (code summarization and method name prediction). We find that around 85% of adaptive triggers in AFRAIDOOR bypass the detection in the defense process. By contrast, only less than 12% of the triggers from previous work bypass the defense. When the defense method is not applied, both AFRAIDOOR and baselines have almost perfect attack success rates. However, once a defense is applied, the success rates of baselines decrease dramatically to 10.47% and 12.06%, while the success rate of AFRAIDOOR are 77.05% and 92.98% on the two tasks. Our finding exposes security weaknesses in code models under stealthy backdoor attacks and shows that the state-of-the-art defense method cannot provide sufficient protection. We call for more research efforts in understanding security threats to code models and developing more effective countermeasures.",
                  "link": "https://arxiv.org/pdf/2301.02496"
                },
                {
                  "id": 6,
                  "title": "Poison Attack and Defense on Deep Source Code Processing Models",
                  "authors": "Authors: Jia Li, Zhuo Li, Huangzhao Zhang, Ge Li, Zhi Jin, Xing Hu, Xin Xia",
                  "imgSrc": "images/class2_papers/codepoisoner.png",
                  "description": "In the software engineering community, deep learning (DL) has recently been applied to many source code processing tasks. Due to the poor interpretability of DL models, their security vulnerabilities require scrutiny. Recently, researchers have identified an emergent security threat, namely poison attack. The attackers aim to inject insidious backdoors into models by poisoning the training data with poison samples. Poisoned models work normally with clean inputs but produce targeted erroneous results with poisoned inputs embedded with triggers. By activating backdoors, attackers can manipulate the poisoned models in security-related scenarios.\n\
                                  To verify the vulnerability of existing deep source code processing models to the poison attack, we present a poison attack framework for source code named CodePoisoner as a strong imaginary enemy. CodePoisoner can produce compilable even human-imperceptible poison samples and attack models by poisoning the training data with poison samples. To defend against the poison attack, we further propose an effective defense approach named CodeDetector to detect poison samples in the training data. CodeDetector can be applied to many model architectures and effectively defend against multiple poison attack approaches. We apply our CodePoisoner and CodeDetector to three tasks, including defect detection, clone detection, and code repair. The results show that (1) CodePoisoner achieves a high attack success rate (max: 100%) in misleading models to targeted erroneous behaviors. It validates that existing deep source code processing models have a strong vulnerability to the poison attack. (2) CodeDetector effectively defends against multiple poison attack approaches by detecting (max: 100%) poison samples in the training data. We hope this work can help practitioners notice the poison attack and inspire the design of more advanced defense techniques.",
                  "link": "https://arxiv.org/pdf/2210.17029"
                },
                {
                  "id": 7,
                  "title": "You See What I Want You to See: Poisoning Vulnerabilities in Neural Code Search",
                  "authors": "Authors: Yao Wan, Shijie Zhang, Hongyu Zhang, Yulei Sui, Guandong Xu, Dezhong Yao, Hai Jin, Lichao Sun",
                  "imgSrc": "images/class2_papers/2022fse.png",
                  "description": "Searching and reusing code snippets from open-source software repositories based on natural-language queries can greatly improve programming productivity.Recently, deep-learning-based approaches have become increasingly popular for code search. Despite substantial progress in training accurate models of code search, the robustness of these models has received little attention so far. \n\
                                  In this paper, we aim to study and understand the security and robustness of code search models by answering the following question: Can we inject backdoors into deep-learning-based code search models? If so, can we detect poisoned data and remove these backdoors? This work studies and develops a series of backdoor attacks on the deep-learning-based models for code search, through data poisoning. We first show that existing models are vulnerable to data-poisoning-based backdoor attacks. We then introduce a simple yet effective attack on neural code search models by poisoning their corresponding training dataset.\n\
                                  Moreover, we demonstrate that attacks can also influence the ranking of the code search results by adding a few specially-crafted source code files to the training corpus. We show that this type of backdoor attack is effective for several representative deep-learning-based code search systems, and can successfully manipulate the ranking list of searching results. Taking the bidirectional RNN-based code search system as an example, the normalized ranking of the target candidate can be significantly raised from top 50% to top 4.43%, given a query containing an attacker targeted word, e.g., file. To defend a model against such attack, we empirically examine an existing popular defense strategy and evaluate its performance. Our results show the explored defense strategy is not yet effective in our proposed backdoor attack for code search systems.",
                  "link": "https://yuleisui.github.io/publications/fse22a.pdf"
                },
                {
                  "id": 8,
                  "title": "PELICAN: Exploiting Backdoors of Naturally Trained Deep Learning Models In Binary Code Analysis",
                  "authors": "Authors: Zhuo Zhang, Guanhong Tao, Guangyu Shen, Shengwei An, Qiuling Xu, Yingqi Liu, Yapeng Ye, Yaoxuan Wu, Xiangyu Zhang",
                  "imgSrc": "images/class2_papers/pelican.png",
                  "description": "Deep Learning (DL) models are increasingly used in many cyber-security applications and achieve superior performance compared to traditional solutions. In this paper, we study backdoor vulnerabilities in naturally trained models used in binary analysis. These backdoors are not injected by attackers but rather products of defects in datasets and/or training processes. The attacker can exploit these vulnerabilities by injecting some small fixed input pattern (e.g., an instruction) called backdoor trigger to their input (e.g., a binary code snippet for a malware detection DL model) such that misclassification can be induced (e.g., the malware evades the detection). We focus on transformer models used in binary analysis. Given a model, we leverage a trigger inversion technique particularly designed for these models to derive trigger instructions that can induce misclassification. During attack, we utilize a novel trigger injection technique to insert the trigger instruction(s) to the input binary code snippet. The injection makes sure that the code snippets' original program semantics are preserved and the trigger becomes an integral part of such semantics and hence cannot be easily eliminated. We evaluate our prototype PELICAN on 5 binary analysis tasks and 15 models. The results show that PELICAN can effectively induce misclassification on all the evaluated models in both white-box and black-box scenarios. Our case studies demonstrate that PELICAN can exploit the backdoor vulnerabilities of two closed-source commercial tools.",
                  "link": "https://www.usenix.org/system/files/usenixsecurity23-zhang-zhuo-pelican.pdf"
                }
          ],
      "totalPages": 5
    }

    const itemsPerPage = 2;
    let currentPage = 1;

    const contentArea = document.getElementById('contentArea');
    const pagination = document.getElementById('pagination');

    // function fetchPapers(page) {
    //   const startIndex = (page - 1) * itemsPerPage;

      
    //   loadContent(jsondata.papers);
    //   updatePagination(jsondata.totalPages);

      

    //   // const url = `https://api.example.com/papers?page=${page}&limit=${itemsPerPage}`;

    //   // fetch('https://run.mocky.io/v3/d9a5ab25-cc82-46c4-be1f-afba0cfcf9c0')
    //   //   .then(response => response.json())
    //   //   .then(data => {
    //   //     loadContent(data.papers);
    //   //     updatePagination(5);
    //   //   })
    //   //   .catch(error => console.error('Error fetching papers:', error));
    // }

    // 获取特定页码的数据


    function fetchPapers(pageNumber) {
      currentPage = pageNumber; // 更新当前页码
      const page_data = paginate(jsonData.papers, itemsPerPage, pageNumber);
      loadContent(page_data); // 加载内容
      updatePagination(); // 更新分页
    }   
    
    // 分页函数
    function paginate(papers, page_size, page_number) {
      let start = page_size * (page_number - 1);
      let end = start + page_size;
      return papers.slice(start, end);
    }
    
    function loadContent(papers) {
      contentArea.innerHTML = ''; // 清空当前内容
      papers.forEach(paper => {
        const row = document.createElement('div');
        row.className = 'row';
        row.innerHTML = `
          <div class="col-md-5">
            <div class="img-box">
              <img src="${paper.imgSrc}" alt="${paper.title}">
            </div>
          </div>
          <div class="col-md-7">
            <div class="detail-box">
              <div class="heading_container">
                <h2>${paper.title}</h2>
              </div>
              <p>${paper.authors}</p>
              <p class="content">${paper.description}</p>
              <div>
                <a href="${paper.link}">Read More</a>
              </div>
            </div>
          </div>
        `;
        contentArea.appendChild(row);
      });
    }

    function updatePagination() {
      pagination.innerHTML = ''; // 清空当前分页链接

      // 创建“上一页”按钮
      if (currentPage > 1) {
        const prevButton = document.createElement('li');
        prevButton.className = 'page-item';
        const prevLink = document.createElement('a');
        prevLink.className = 'page-link';
        prevLink.textContent = 'Prev';
        prevLink.addEventListener('click', function(event) {
          event.preventDefault(); // 阻止默认行为
          fetchPapers(currentPage - 1); // 加载新页面的内容
        });
        prevButton.appendChild(prevLink);
        pagination.appendChild(prevButton);
      }
      
      const totalPages = Math.ceil(jsonData.papers.length / itemsPerPage);
      for (let i = 1; i <= totalPages; i++) {
        const pageItem = document.createElement('li');
        pageItem.className = 'page-item' + (i === currentPage ? ' active' : '');
        const pageLink = document.createElement('a');
        pageLink.className = 'page-link';
        pageLink.setAttribute('href', '#');
        pageLink.textContent = i;

        // 添加点击事件监听器
        pageLink.addEventListener('click', function(event) {
          event.preventDefault(); // 阻止默认行为
          fetchPapers(i); // 加载新页面的内容
        });

        pageLink.onclick = function() { fetchPapers(i); };
        pageItem.appendChild(pageLink);
        pagination.appendChild(pageItem);
        // console.log(pagination.innerHTML);
      }

        // 创建“下一页”按钮
      if (currentPage < totalPages) {
        const nextButton = document.createElement('li');
        nextButton.className = 'page-item';
        const nextLink = document.createElement('a');
        nextLink.className = 'page-link';
        nextLink.textContent = 'Next';
        nextLink.addEventListener('click', function(event) {
          event.preventDefault(); // 阻止默认行为
          fetchPapers(currentPage + 1); // 加载新页面的内容
        });

        nextButton.appendChild(nextLink);
        pagination.appendChild(nextButton);
      }

    }
    // 初始化分页
    updatePagination();

    // 初始化加载第一页内容
    fetchPapers(1);
  </script>

  <!-- end owl carousel script -->

</body>

</html>